{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('results_ensemble/csv/soap_categorized_phrases_phrase_set_0.csv')\n",
    "\n",
    "# Extract the phrases and their corresponding main clusters\n",
    "phrases = df['phrase'].tolist()\n",
    "main_clusters = df['main_cluster'].tolist()\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(main_clusters, method='ward')\n",
    "\n",
    "# Create labels for the dendrogram\n",
    "dendrogram_labels = [f\"{cluster}: {phrase}\" for cluster, phrase in zip(main_clusters, phrases)]\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(20, 10))\n",
    "dendrogram(linkage_matrix, labels=dendrogram_labels, leaf_rotation=90)\n",
    "plt.title(\"Dendrogram of SOAP Categorized Phrases\")\n",
    "plt.xlabel(\"Phrase\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, inconsistent\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def perform_subclustering_dendrogram(embeddings, cluster_labels, cluster_to_soap, max_depth=3):\n",
    "    \"\"\"\n",
    "    Perform recursive subclustering for each cluster using dendrogram structure.\n",
    "    The number of subclusters is determined automatically using the inconsistency method.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): Array of embeddings\n",
    "        cluster_labels (numpy.ndarray): Cluster labels\n",
    "        cluster_to_soap (dict): Mapping of clusters to SOAP categories\n",
    "        max_depth (int): Maximum depth of recursive subclustering\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of phrase index to subcluster label\n",
    "    \"\"\"\n",
    "    def recursive_subcluster(embeddings_subset, indices, current_depth=0):\n",
    "        if len(indices) < 2 or current_depth >= max_depth:\n",
    "            return {idx: 0 for idx in indices}\n",
    "        \n",
    "        # Compute linkage for this subset\n",
    "        linkage_sub = linkage(embeddings_subset, method='ward')\n",
    "        \n",
    "        # Use inconsistency to determine a threshold\n",
    "        incons = inconsistent(linkage_sub)\n",
    "        heights = incons[:, -1]\n",
    "        threshold = np.mean(heights) + np.std(heights)\n",
    "        \n",
    "        # Get subcluster labels\n",
    "        sub_labels = fcluster(linkage_sub, t=threshold, criterion='distance')\n",
    "        \n",
    "        # If we only got one cluster, no need to recurse\n",
    "        if len(np.unique(sub_labels)) == 1:\n",
    "            return {idx: 0 for idx in indices}\n",
    "        \n",
    "        # Recursively subcluster each subcluster\n",
    "        result = {}\n",
    "        for sub_label in np.unique(sub_labels):\n",
    "            sub_mask = sub_labels == sub_label\n",
    "            sub_indices = indices[sub_mask]\n",
    "            sub_embeddings = embeddings_subset[sub_mask]\n",
    "            \n",
    "            # Recursive call with incremented depth\n",
    "            sub_results = recursive_subcluster(sub_embeddings, sub_indices, current_depth + 1)\n",
    "            \n",
    "            # Combine results with proper subcluster numbering\n",
    "            for idx, sub_sub_label in sub_results.items():\n",
    "                result[idx] = int(f\"{sub_label}{sub_sub_label}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # Group indices by cluster\n",
    "    cluster_indices = defaultdict(list)\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        cluster_indices[label].append(i)\n",
    "    \n",
    "    # Perform recursive subclustering for each cluster\n",
    "    subcluster_labels = {}\n",
    "    for cluster, indices in cluster_indices.items():\n",
    "        if len(indices) < 2:\n",
    "            subcluster_labels[indices[0]] = 0\n",
    "            continue\n",
    "            \n",
    "        # Extract embeddings for this cluster\n",
    "        cluster_embeddings = embeddings[indices]\n",
    "        indices_array = np.array(indices)\n",
    "        \n",
    "        # Perform recursive subclustering\n",
    "        cluster_results = recursive_subcluster(cluster_embeddings, indices_array)\n",
    "        subcluster_labels.update(cluster_results)\n",
    "    \n",
    "    return subcluster_labels\n",
    "\n",
    "# Example usage in your pipeline:\n",
    "# subcluster_labels = perform_subclustering_dendrogram(combined_embeddings, cluster_labels, cluster_to_soap, linkage_matrix, phrases) \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from bert_score import BERTScorer\n",
    "# from bleurt import score\n",
    "import os\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the key phrases data\n",
    "def load_key_phrases(file_path):\n",
    "    \"\"\"\n",
    "    Load key phrases from CSV file and preprocess them.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing key phrases\n",
    "\n",
    "    Returns:\n",
    "        list: List of preprocessed key phrases\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract individual key phrases from the text\n",
    "    all_phrases = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract phrases between quotes\n",
    "        phrases = re.findall(r'\"([^\"]*)\"', row['cleaned_key_phrases'])\n",
    "        # Clean up phrases\n",
    "        phrases = [p.strip() for p in phrases if len(p.strip()) > 10]  # Filter out very short phrases\n",
    "        all_phrases.append(phrases)\n",
    "    print(f'All phrases: {len(all_phrases)}')\n",
    "    return all_phrases\n",
    "\n",
    "# Load the model and tokenizer for embeddings\n",
    "def load_model_and_tokenizer(model_name=\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer for generating embeddings.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to use\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tokenizer, model)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Generate prompt-aware embeddings\n",
    "def generate_embeddings(phrases, tokenizer, model, prompt_type):\n",
    "    \"\"\"\n",
    "    Generate embeddings for phrases using prompt-aware approach.\n",
    "\n",
    "    Args:\n",
    "        phrases (list): List of phrases to embed\n",
    "        tokenizer: Tokenizer for the model\n",
    "        model: Model for generating embeddings\n",
    "        prompt_type (str): Type of prompt to use (S, O, A, P)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of embeddings\n",
    "    \"\"\"\n",
    "    # Define prompts for each SOAP category\n",
    "    prompts = {\n",
    "        'S': \"Subjective: This section captures the patient's personal experiences and feelings. It includes the chief complaint, history of present illness, and any other relevant personal or family medical history. Example: 'The patient reports a persistent headache for the past three days.'\",\n",
    "        'O': \"Objective: This section records measurable or observable data from the patient encounter, such as vital signs, physical examination findings, and laboratory results. Example: 'Blood pressure is 140/90 mmHg, and the patient has a temperature of 37.5Â°C.'\",\n",
    "        'A': \"Assessment: This section provides a medical diagnosis or assessment based on the subjective and objective information. It includes the clinician's interpretation and analysis of the patient's condition. Example: 'The patient is diagnosed with hypertension based on elevated blood pressure readings.'\",\n",
    "        'P': \"Plan: This section outlines the treatment strategy, including medications, therapies, and follow-up appointments. It details the steps to manage the patient's condition. Example: 'Prescribe lisinopril 10 mg daily and schedule a follow-up in two weeks.'\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for phrase in tqdm(phrases, desc=f\"Generating {prompt_type} embeddings\"):\n",
    "        # Add the appropriate prompt\n",
    "        prompted_phrase = f\"Generate an embedding for {phrase} based on the context {prompts[prompt_type]}, since this the type of phrase it is. Understand from the example and \"\n",
    "\n",
    "        # Tokenize and generate embeddings\n",
    "        inputs = tokenizer(prompted_phrase, return_tensors=\"pt\", truncation=True, max_length=2048, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Use the [CLS] token embedding as the sentence embedding\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(embedding[0])\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "def perform_hierarchical_clustering(embeddings, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): Array of embeddings\n",
    "        n_clusters (int): Number of clusters to form\n",
    "\n",
    "    Returns:\n",
    "        tuple: (clustering model, cluster labels, linkage matrix)\n",
    "    \"\"\"\n",
    "    # Adjust number of clusters if there are fewer samples than requested clusters\n",
    "    n_samples = len(embeddings)\n",
    "    if n_samples < n_clusters:\n",
    "        print(f\"Warning: Only {n_samples} samples available, reducing number of clusters from {n_clusters} to {n_samples}\")\n",
    "        n_clusters = n_samples\n",
    "\n",
    "    # Compute linkage matrix\n",
    "    linkage_matrix = linkage(embeddings, method='ward')\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    cluster_labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "    return clustering, cluster_labels, linkage_matrix\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_clustering(embeddings, cluster_labels):\n",
    "    \"\"\"\n",
    "    Evaluate clustering using multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): Array of embeddings\n",
    "        cluster_labels (numpy.ndarray): Cluster labels\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    n_samples = len(embeddings)\n",
    "    n_labels = len(np.unique(cluster_labels))\n",
    "\n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {}\n",
    "\n",
    "    # Only calculate silhouette score if we have enough samples and labels\n",
    "    if n_samples > 2 and 1 < n_labels < n_samples:\n",
    "        # Silhouette score\n",
    "        metrics['silhouette_score'] = silhouette_score(embeddings, cluster_labels)\n",
    "    else:\n",
    "        # For small samples, use a simple distance-based metric instead\n",
    "        print(f\"Warning: Not enough samples ({n_samples}) or labels ({n_labels}) for silhouette score. Using alternative metric.\")\n",
    "        # Calculate average distance between samples in the same cluster\n",
    "        same_cluster_distances = []\n",
    "        for label in np.unique(cluster_labels):\n",
    "            mask = cluster_labels == label\n",
    "            if np.sum(mask) > 1:  # Only if there are at least 2 samples in this cluster\n",
    "                cluster_embeddings = embeddings[mask]\n",
    "                # Calculate pairwise distances\n",
    "                distances = np.zeros((len(cluster_embeddings), len(cluster_embeddings)))\n",
    "                for i in range(len(cluster_embeddings)):\n",
    "                    for j in range(i+1, len(cluster_embeddings)):\n",
    "                        dist = np.linalg.norm(cluster_embeddings[i] - cluster_embeddings[j])\n",
    "                        distances[i, j] = distances[j, i] = dist\n",
    "                # Average distance for this cluster\n",
    "                same_cluster_distances.append(np.mean(distances[distances > 0]))\n",
    "\n",
    "        # Use the average of same-cluster distances as a proxy for silhouette score\n",
    "        metrics['silhouette_score'] = np.mean(same_cluster_distances) if same_cluster_distances else 0.0\n",
    "\n",
    "    # Calinski-Harabasz index\n",
    "    if n_samples > 2 and n_labels > 1:\n",
    "        metrics['calinski_harabasz_score'] = calinski_harabasz_score(embeddings, cluster_labels)\n",
    "    else:\n",
    "        metrics['calinski_harabasz_score'] = 0.0\n",
    "\n",
    "    # Davies-Bouldin index\n",
    "    if n_samples > 2 and n_labels > 1:\n",
    "        metrics['davies_bouldin_score'] = davies_bouldin_score(embeddings, cluster_labels)\n",
    "    else:\n",
    "        metrics['davies_bouldin_score'] = 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Calculate semantic similarity between clusters\n",
    "def calculate_cluster_similarity(embeddings, cluster_labels):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity between clusters.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): Array of embeddings\n",
    "        cluster_labels (numpy.ndarray): Cluster labels\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Similarity matrix between clusters\n",
    "    \"\"\"\n",
    "    # Calculate centroid for each cluster\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    centroids = {}\n",
    "\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_labels == label\n",
    "        centroids[label] = np.mean(embeddings[mask], axis=0)\n",
    "\n",
    "    # Calculate similarity between centroids\n",
    "    similarity_matrix = np.zeros((len(unique_labels), len(unique_labels)))\n",
    "    for i, label1 in enumerate(unique_labels):\n",
    "        for j, label2 in enumerate(unique_labels):\n",
    "            similarity = cosine_similarity([centroids[label1]], [centroids[label2]])[0][0]\n",
    "            similarity_matrix[i, j] = similarity\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "# Calculate semantic coherence of clusters\n",
    "def calculate_semantic_coherence(embeddings, cluster_labels, phrases):\n",
    "    \"\"\"\n",
    "    Calculate semantic coherence of clusters using BERTScore.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): Array of embeddings\n",
    "        cluster_labels (numpy.ndarray): Cluster labels\n",
    "        phrases (list): List of phrases\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of coherence scores for each cluster\n",
    "    \"\"\"\n",
    "    # Initialize BERTScore\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "    # Calculate coherence for each cluster\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    coherence_scores = {}\n",
    "\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_labels == label\n",
    "        cluster_phrases = [phrases[i] for i in range(len(phrases)) if mask[i]]\n",
    "\n",
    "        if len(cluster_phrases) < 2:\n",
    "            coherence_scores[label] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Calculate pairwise similarities\n",
    "        similarities = []\n",
    "        for i in range(len(cluster_phrases)):\n",
    "            for j in range(i+1, len(cluster_phrases)):\n",
    "                score = bert_scorer.score([cluster_phrases[i]], [cluster_phrases[j]])[2].mean()\n",
    "                similarities.append(score)\n",
    "\n",
    "        # Average similarity as coherence score\n",
    "        coherence_scores[label] = np.mean(similarities)\n",
    "\n",
    "    return coherence_scores\n",
    "\n",
    "# Visualize clustering results with cluster and subcluster labels inside the dendrogram.\n",
    "def visualize_clustering(linkage_matrix, cluster_labels, phrases, output_dir=\"visualizations\"):\n",
    "    \"\"\"\n",
    "    Visualize clustering results with cluster and subcluster labels inside the dendrogram.\n",
    "\n",
    "    Args:\n",
    "        linkage_matrix (numpy.ndarray): Linkage matrix\n",
    "        cluster_labels (numpy.ndarray): Cluster labels\n",
    "        phrases (list): List of phrases\n",
    "        output_dir (str): Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a mapping of cluster and subcluster to phrases\n",
    "    cluster_subcluster_phrases = defaultdict(lambda: defaultdict(list))\n",
    "    for i, (cluster, phrase) in enumerate(zip(cluster_labels, phrases)):\n",
    "        # Get subcluster from the linkage matrix\n",
    "        subcluster = fcluster(linkage_matrix, t=1, criterion='maxclust')[i]\n",
    "        cluster_subcluster_phrases[cluster][subcluster].append(phrase)\n",
    "\n",
    "    # Function to create labels for each node\n",
    "    def label_func(x, pos):\n",
    "        if x < len(cluster_labels):\n",
    "            cluster = cluster_labels[int(x)]\n",
    "            subcluster = fcluster(linkage_matrix, t=1, criterion='maxclust')[int(x)]\n",
    "            return f\"{cluster}-{subcluster}\"\n",
    "        return \"\"\n",
    "\n",
    "    # Plot dendrogram with custom labels\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    dendrogram(linkage_matrix, \n",
    "               leaf_label_func=label_func,\n",
    "               leaf_rotation=0,\n",
    "               show_leaf_counts=True)\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram with Cluster-Subcluster Labels\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(output_dir, \"dendrogram_1.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot cluster distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=cluster_labels)\n",
    "    plt.title(\"Distribution of Clusters\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"cluster_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Visualize embeddings using dimensionality reduction\n",
    "def visualize_embeddings(embeddings, cluster_labels, phrases, output_dir=\"visualizations\"):\n",
    "    \"\"\"\n",
    "    Visualize embeddings using dimensionality reduction techniques.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): Array of embeddings\n",
    "        cluster_labels (numpy.ndarray): Cluster labels\n",
    "        phrases (list): List of phrases\n",
    "        output_dir (str): Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot t-SNE\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=cluster_labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(\"t-SNE Visualization of Embeddings\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"tsne_visualization.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Apply UMAP\n",
    "    umap_reducer = umap.UMAP(random_state=42)\n",
    "    umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Plot UMAP\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=cluster_labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(\"UMAP Visualization of Embeddings\")\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"umap_visualization.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Plot PCA\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], c=cluster_labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(\"PCA Visualization of Embeddings\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"pca_visualization.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def map_clusters_to_soap(cluster_labels, embeddings, phrases):\n",
    "    \"\"\"\n",
    "    Ensemble method for mapping clusters to SOAP categories using:\n",
    "    1. Prompt-based embedding comparison\n",
    "    2. Topic modeling\n",
    "    3. Zero/few-shot LLM classification\n",
    "    Returns per-method and majority-vote assignments.\n",
    "    Ensures one-to-one mapping between clusters and SOAP categories for each method.\n",
    "    \"\"\"\n",
    "    # --- 1. Prompt-based Embedding Comparison ---\n",
    "    soap_prompts = {\n",
    "        'S': \"Subjective: This section captures the patient's personal experiences and feelings. It includes the chief complaint, history of present illness, and any other relevant personal or family medical history. Example: 'The patient reports a persistent headache for the past three days.'\",\n",
    "        'O': \"Objective: This section records measurable or observable data from the patient encounter, such as vital signs, physical examination findings, and laboratory results. Example: 'Blood pressure is 140/90 mmHg, and the patient has a temperature of 37.5Â°C.'\",\n",
    "        'A': \"Assessment: This section provides a medical diagnosis or assessment based on the subjective and objective information. It includes the clinician's interpretation and analysis of the patient's condition. Example: 'The patient is diagnosed with hypertension based on elevated blood pressure readings.'\",\n",
    "        'P': \"Plan: This section outlines the treatment strategy, including medications, therapies, and follow-up appointments. It details the steps to manage the patient's condition. Example: 'Prescribe lisinopril 10 mg daily and schedule a follow-up in two weeks.'\"\n",
    "    }\n",
    "    tokenizer, model = load_model_and_tokenizer()\n",
    "    prompt_embeds = {}\n",
    "    for k, v in soap_prompts.items():\n",
    "        inputs = tokenizer(v, return_tensors=\"pt\", truncation=True, max_length=256, padding=True)\n",
    "        inputs = {k2: v2.to(model.device) for k2, v2 in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        prompt_embeds[k] = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]\n",
    "    # Compute cluster centroids\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    cluster_centroids = {label: np.mean(embeddings[cluster_labels == label], axis=0) for label in unique_labels}\n",
    "    soap_keys = list(soap_prompts.keys())\n",
    "    n_clusters = len(unique_labels)\n",
    "    n_soap = len(soap_keys)\n",
    "    # --- Prompt-based Hungarian assignment ---\n",
    "    prompt_score_matrix = np.zeros((n_clusters, n_soap))\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        centroid = cluster_centroids[label]\n",
    "        for j, soap in enumerate(soap_keys):\n",
    "            prompt_score_matrix[i, j] = cosine_similarity([centroid], [prompt_embeds[soap]])[0][0]\n",
    "    # Hungarian: maximize similarity (minimize negative similarity)\n",
    "    row_ind, col_ind = linear_sum_assignment(-prompt_score_matrix)\n",
    "    prompt_assignments = {unique_labels[i]: soap_keys[j] for i, j in zip(row_ind, col_ind)}\n",
    "\n",
    "    # --- 2. Topic Modeling (NMF) Hungarian assignment ---\n",
    "    topic_keywords = {\n",
    "        'S': ['report', 'feel', 'describe', 'complain', 'history', 'symptom'],\n",
    "        'O': ['exam', 'vital', 'lab', 'result', 'sign', 'physical', 'imaging'],\n",
    "        'A': ['diagnosis', 'assessment', 'impression', 'evaluation'],\n",
    "        'P': ['plan', 'treatment', 'recommend', 'prescribe', 'follow-up', 'therapy']\n",
    "    }\n",
    "    topic_score_matrix = np.zeros((n_clusters, n_soap))\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        cluster_phrases = [phrases[k] for k in range(len(phrases)) if cluster_labels[k] == label]\n",
    "        if len(cluster_phrases) < 2:\n",
    "            topic_score_matrix[i, :] = 0\n",
    "            topic_score_matrix[i, 0] = 1  # fallback: S\n",
    "            continue\n",
    "        vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "        X = vectorizer.fit_transform(cluster_phrases)\n",
    "        nmf = NMF(n_components=1, random_state=42)\n",
    "        W = nmf.fit_transform(X)\n",
    "        H = nmf.components_[0]\n",
    "        top_indices = H.argsort()[::-1][:5]\n",
    "        top_words = [vectorizer.get_feature_names_out()[idx] for idx in top_indices]\n",
    "        for j, soap in enumerate(soap_keys):\n",
    "            topic_score_matrix[i, j] = sum(word in topic_keywords[soap] for word in top_words)\n",
    "    row_ind, col_ind = linear_sum_assignment(-topic_score_matrix)\n",
    "    topic_assignments = {unique_labels[i]: soap_keys[j] for i, j in zip(row_ind, col_ind)}\n",
    "\n",
    "    # --- 3. Zero/Few-shot LLM Classification Hungarian assignment ---\n",
    "    llm_score_matrix = np.zeros((n_clusters, n_soap))\n",
    "    try:\n",
    "        zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        candidate_labels = [\"Subjective\", \"Objective\", \"Assessment\", \"Plan\"]\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            cluster_phrases = [phrases[k] for k in range(len(phrases)) if cluster_labels[k] == label]\n",
    "            rep_phrase = max(cluster_phrases, key=len)\n",
    "            result = zero_shot(rep_phrase, candidate_labels)\n",
    "            for j, soap in enumerate(soap_keys):\n",
    "                # Use the score for the corresponding label\n",
    "                label_idx = result['labels'].index({\n",
    "                    'S': 'Subjective', 'O': 'Objective', 'A': 'Assessment', 'P': 'Plan'\n",
    "                }[soap])\n",
    "                llm_score_matrix[i, j] = result['scores'][label_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"LLM zero-shot classification failed: {e}\")\n",
    "        llm_score_matrix[:, 0] = 1  # fallback: all S\n",
    "    row_ind, col_ind = linear_sum_assignment(-llm_score_matrix)\n",
    "    llm_assignments = {unique_labels[i]: soap_keys[j] for i, j in zip(row_ind, col_ind)}\n",
    "\n",
    "    # --- 4. Ensemble Hungarian assignment ---\n",
    "    # For ensemble, use majority voting as a score matrix\n",
    "    ensemble_score_matrix = np.zeros((n_clusters, n_soap))\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        votes = [prompt_assignments[label], topic_assignments[label], llm_assignments[label]]\n",
    "        for j, soap in enumerate(soap_keys):\n",
    "            ensemble_score_matrix[i, j] = votes.count(soap)\n",
    "    row_ind, col_ind = linear_sum_assignment(-ensemble_score_matrix)\n",
    "    ensemble_assignments = {unique_labels[i]: soap_keys[j] for i, j in zip(row_ind, col_ind)}\n",
    "\n",
    "    # --- Return all assignments for interpretability ---\n",
    "    return {\n",
    "        'prompt': prompt_assignments,\n",
    "        'topic': topic_assignments,\n",
    "        'llm': llm_assignments,\n",
    "        'ensemble': ensemble_assignments\n",
    "    }\n",
    "\n",
    "def convert_keys_to_str(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {str(k): convert_keys_to_str(v) for k, v in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_keys_to_str(i) for i in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "def process_phrase_set(phrases, phrase_set_index, tokenizer, model, output_base_dir=\"results_ensemble\"):\n",
    "    os.makedirs(f\"{output_base_dir}/dendrograms\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_base_dir}/embeddings\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_base_dir}/json\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_base_dir}/csv\", exist_ok=True)\n",
    "    print(f\"Processing phrase set {phrase_set_index} with {len(phrases)} phrases\")\n",
    "    # Generate embeddings for each SOAP category\n",
    "    all_embeddings = {}\n",
    "    for prompt_type in tqdm(['S', 'O', 'A', 'P'], desc=\"Generating embeddings\", position=0, leave=True):\n",
    "        embeddings = generate_embeddings(phrases, tokenizer, model, prompt_type)\n",
    "        all_embeddings[prompt_type] = embeddings\n",
    "    combined_embeddings = np.mean([emb for emb in all_embeddings.values()], axis=0)\n",
    "    clustering, cluster_labels, linkage_matrix = perform_hierarchical_clustering(combined_embeddings)\n",
    "    soap_assignments = map_clusters_to_soap(cluster_labels, combined_embeddings, phrases)\n",
    "    # Use ensemble for downstream, but save all\n",
    "    cluster_to_soap = soap_assignments['ensemble']\n",
    "    subcluster_labels = perform_subclustering_dendrogram(combined_embeddings, cluster_labels, cluster_to_soap)\n",
    "    evaluation_metrics = evaluate_clustering(combined_embeddings, cluster_labels)\n",
    "    similarity_matrix = calculate_cluster_similarity(combined_embeddings, cluster_labels)\n",
    "    coherence_scores = calculate_semantic_coherence(combined_embeddings, cluster_labels, phrases)\n",
    "    visualize_clustering(\n",
    "        linkage_matrix,\n",
    "        cluster_labels,\n",
    "        phrases,\n",
    "        output_dir=f\"{output_base_dir}/dendrograms/phrase_set_{phrase_set_index}\"\n",
    "    )\n",
    "    results_df = pd.DataFrame({\n",
    "        'phrases': phrases,\n",
    "        'cluster_labels': cluster_labels,\n",
    "    })\n",
    "    results_df.to_csv(f'{output_base_dir}/csv/clustering_results_phrase_set_{phrase_set_index}.csv', index=False)\n",
    "    soap_phrases_list = []\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        soap_category = cluster_to_soap[label]\n",
    "        subcluster = subcluster_labels[i]\n",
    "        soap_phrases_list.append({\n",
    "            'soap_category': soap_category,\n",
    "            'main_cluster': label,\n",
    "            'subcluster': subcluster,\n",
    "            'phrase': phrases[i]\n",
    "        })\n",
    "    soap_phrases_df = pd.DataFrame(soap_phrases_list)\n",
    "    soap_phrases_df.to_csv(f'{output_base_dir}/csv/soap_categorized_phrases_phrase_set_{phrase_set_index}.csv', index=False)\n",
    "    cluster_phrases = defaultdict(lambda: defaultdict(list))\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        main_cluster = label\n",
    "        subcluster = subcluster_labels[i]\n",
    "        cluster_phrases[str(main_cluster)][str(subcluster)].append(phrases[i])\n",
    "    results = {\n",
    "        'phrases': phrases,\n",
    "        'cluster_labels': cluster_labels.tolist(),\n",
    "        'subcluster_labels': subcluster_labels,\n",
    "        'soap_assignments': convert_keys_to_str(soap_assignments),\n",
    "        'cluster_to_soap': {str(k): v for k, v in cluster_to_soap.items()},\n",
    "        'evaluation_metrics': evaluation_metrics,\n",
    "        'similarity_matrix': similarity_matrix.tolist(),\n",
    "        'coherence_scores': {str(k): v for k, v in coherence_scores.items()},\n",
    "        'cluster_phrases': cluster_phrases\n",
    "    }\n",
    "    def default_serializer(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n",
    "    with open(f'{output_base_dir}/json/results_phrase_set_{phrase_set_index}.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=default_serializer)\n",
    "    print(f\"Results for phrase set {phrase_set_index} saved to {output_base_dir}/\")\n",
    "    return results \n",
    "\n",
    "def main_hc():\n",
    "    # Load key phrases\n",
    "    all_phrases = load_key_phrases(\"key_phrases_results.csv\")\n",
    "\n",
    "    print(f\"Loaded {len(all_phrases)} phrase sets\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer, model = load_model_and_tokenizer()\n",
    "\n",
    "    # Process each phrase set\n",
    "    all_results = []\n",
    "    for i, phrases in tqdm(enumerate(all_phrases[0:4]), desc=\"Processing phrase sets\"):\n",
    "        results = process_phrase_set(phrases, i, tokenizer, model)\n",
    "        all_results.append(results)\n",
    "\n",
    "    # Save summary of all results\n",
    "    summary = {\n",
    "        'total_phrase_sets': len(all_phrases),\n",
    "        'phrase_set_sizes': [len(result['phrases']) for result in all_results],\n",
    "        'average_evaluation_metrics': {\n",
    "            metric: np.mean([result['evaluation_metrics'][metric] for result in all_results])\n",
    "            for metric in all_results[0]['evaluation_metrics'].keys()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('results/json/summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"All phrase sets processed. Summary saved to results/json/summary.json\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
