# -*- coding: utf-8 -*-
"""Copy of Clinical Note Generation Side by Side

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qcpacxfKIXWySUX-zEy42nwllh-L6VKv
"""

from src.components.step3_keyPhraseExtraction.kpe_for_ft import normalize_medical_data
prompt_header = """
Generate a concise medical note that captures the key clinical information from this doctor-patient interaction.
Focus on extracting the most important medical details, symptoms, diagnoses, and treatment plans.
The note should be clear, professional, and follow standard medical documentation practices.
"""

import pandas as pd

# Load the CSV file
df = pd.read_csv('sample-dialogue-and-notes.csv')

prompts=[]
# Traverse each row
for idx, row in df.iterrows():
    dialogue = row['dialogue']
    note = row['note']
    dialogue, note = normalize_medical_data(dialogue, note)
    # Now you can use dialogue and note to build your prompt or process as needed
    print(f"Dialogue: {dialogue}\nNote: {note}\n")
    prompts.append( "Transcript: " + dialogue + "\nNote: " + note + "\n\n" + prompt_header)

from transformers import AutoModelForCausalLM, AutoTokenizer

base_model_name = "Qwen/Qwen3-4B"

# load the tokenizer and the model
base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype="auto",
    device_map="auto"
)

novel_model_name = "ClinicianFOCUS/Clinician-Note-2.0a"

# load the tokenizer and the model
novel_tokenizer = AutoTokenizer.from_pretrained(novel_model_name)
novel_model = AutoModelForCausalLM.from_pretrained(
    novel_model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
messages = [
    {"role": "user", "content": prompts[0]}
]
base_text = base_tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
)
base_model_inputs = base_tokenizer([base_text], return_tensors="pt").to(base_model.device)

# conduct text completion
generated_ids = base_model.generate(
    **base_model_inputs,
    max_new_tokens=32768,
    temperature=0.3,
    top_p=0.9,
    top_k=20,
)
output_ids = generated_ids[0][len(base_model_inputs.input_ids[0]):].tolist()

# parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = base_tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = base_tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("Base SOAP Note Generator:\n\n")
print("thinking content:", thinking_content)
print("content:", content)

novel_text = novel_tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
)
model_inputs = novel_tokenizer([novel_text], return_tensors="pt").to(novel_model.device)

# conduct text completion
generated_ids = novel_model.generate(
    **model_inputs,
    max_new_tokens=32768,
    temperature=0.3,
    top_p=0.9,
    top_k=20,
)
novel_output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

# parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(novel_output_ids) - novel_output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = novel_tokenizer.decode(novel_output_ids[:index], skip_special_tokens=True).strip("\n")
content = novel_tokenizer.decode(novel_output_ids[index:], skip_special_tokens=True).strip("\n")

print("Novel SOAP Note Generator:\n\n")
print("thinking content:", thinking_content)
print("content:", content)
print(f"\n\n\n Transcript: {dialogue}")